{"cells":[{"outputs":[],"execution_count":20,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"40C6433C8916486A8426F76F0E9EF06B","scrolled":false,"hide_input":true}},{"outputs":[],"execution_count":21,"source":"# 查看当前kernerl下的package\n# !pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"25597C20F9B94F98A81620716659D496","scrolled":false,"hide_input":true}},{"outputs":[],"execution_count":22,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"91EA521F053D4C108E48873F464FC0C3","scrolled":false,"hide_input":false}},{"metadata":{"id":"C0A7C11DCF1841DB8FD2B63D150D3444","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport time\nimport math\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn import metrics\nfrom sklearn import utils\nfrom sklearn.model_selection import KFold\nfrom fastText import train_supervised\nimport lightgbm as lgb\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument, LabeledSentence\nfrom tqdm import tqdm\nimport Levenshtein\nimport gc","execution_count":23},{"metadata":{"id":"E4098633BDB341BF8E07397B0EAC4B13","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":"path=\"/home/kesci/input/bytedance/first-round/\"\ntrain = pd.read_csv(path+'train.csv', header=None, chunksize=10000000)\ntest = pd.read_csv(path+'test.csv', header=None)\npre_df = test[[0, 2]]","execution_count":24},{"metadata":{"id":"7AA642F259554E83B9FB1657DC24AAE3","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"def q_auc(df):\r\n    invalid_count = 0\r\n    query_ids = df['query_id'].unique()\r\n    aucs = []\r\n    for query_id in query_ids:\r\n        current = df[df['query_id'] == query_id]\r\n        if sum(current['label']) not in [0, len(current['label'])]:\r\n            auc = roc_auc_score(current['label'], current['prediction'])  # 这里计算qauc\r\n        else:\r\n            auc = 0.5\r\n            invalid_count += 1\r\n        aucs.append(auc)\r\n    print(f'q_auc invalid_count = {invalid_count}')\r\n    return np.mean(aucs)","execution_count":25},{"metadata":{"id":"99CA1B7AA3BF4C58838F80C9B94C5894","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"# ct = CountVectorizer()\n# d = train[1] + ' ' +train[3]\n# x_train = ct.fit_transform(d)\n\n# d = test[1] + ' ' +test[3]\n# x_test = ct.transform(d)\n\n# del d","execution_count":26},{"metadata":{"id":"5FDA98A54E5D4F4482AF01CC47FA06F7","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"\n# #lr+CountVectorizer\n\n# # def getfTitleOhot(df):\n# #     a = np.zeros((df.shape[0], 20), dtype=np.int8)\n# #     for i in range(len(d)):\n# #         b = d[i].split(' ')\n# #         for j in b:\n# #             a[i][int(j)] = int(j)\n            \n# #     return a\n\n# kf_n = 5\n# kf = KFold(n_splits=kf_n, shuffle=True, random_state=2)\n# pre = 0\n\n# # model = SGDClassifier(penalty='l1', random_state = 2, shuffle = True, loss='log', alpha=0.01)\n# model = LogisticRegression(penalty='l1', C=0.14, n_jobs=-1)\n# x = 1\n# for df in train:\n    \n#     ct = CountVectorizer(ngram_range=(1, 3), min_df=50)\n#     # ct = CountVectorizer(ngram_range=(1, 2))\n#     # ct = TfidfVectorizer(sublinear_tf=True,ngram_range=(1, 2), max_df=0.5, analyzer='word', norm='l1')\n    \n#     print('第%d次训练开始'%x)\n    \n#     #训练集\n#     d = df[1] + ' ' +df[3]\n#     y_train = df[4].copy()\n#     x_train = ct.fit_transform(d)\n    \n#     # title_oneh = np.array(pd.get_dummies(df[2]))\n#     # x_train = np.hstack((x_train, title_oneh))\n#     # del title_oneh\n        \n#     print('训练集处理完成')\n\n#     #测试集\n#     d = test[1] + ' ' +test[3]\n#     x_test = ct.transform(d)\n    \n#     # title_oneh = np.array(pd.get_dummies(test[2]))\n#     # x_train = np.hstack((x_train, title_oneh))\n#     # del title_oneh\n    \n#     print('测试集处理完成')\n    \n#     del d\n\n        \n#     for x_tr, x_vail in kf.split(x_train):\n#         x_t1 = x_train[x_tr]\n#         y_t1 = y_train.iloc[x_tr]\n        \n#         x_t2 = x_train[x_vail]\n#         y_t2 = y_train.iloc[x_vail]\n        \n#         model.fit(x_t1, y_t1)\n#         print(metrics.roc_auc_score(y_t2, model.predict_proba(x_t2)[:, 1]))\n#         pre += model.predict_proba(x_test)[:, 1]\n        \n#     # model.fit(x_train, y_train)\n#     # pre += model.predict_proba(x_test)[:, 1]\n        \n#     del x_train\n#     del x_test\n#     del ct\n    \n#     print('第%d次训练结束'%x)\n\n#     x += 1\n        \n\n    \n# pre1 = pre/(kf_n*(x-1))\n# # pre1 = pre/(x-1)\n\n# pre_df = pre_df.rename(columns={0:'query_id', 2:'query_title_id'})\n# pre_df['prediction'] = pre1\n# pre_df.to_csv('lr_ct_500wEoch_无cv.csv', index=False, header=False)\n\n# '''\n# 5折交叉验证+500w一次,验证得分0.574\n# 2折交叉验证+1000w一次,验证得分0.568\n# '''","execution_count":27},{"metadata":{"id":"A77B4F2E8D7F49A68264D81A4F06C6F5","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"def lcsubstr_lens(s1, s2): \r\n    m=[[0 for i in range(len(s2)+1)]  for j in range(len(s1)+1)]  #生成0矩阵，为方便后续计算，比字符串长度多了一列\r\n    mmax=0   #最长匹配的长度\r\n    p=0  #最长匹配对应在s1中的最后一位\r\n    for i in range(len(s1)):\r\n        for j in range(len(s2)):\r\n            if s1[i]==s2[j]:\r\n                m[i+1][j+1]=m[i][j]+1\r\n                if m[i+1][j+1]>mmax:\r\n                    mmax=m[i+1][j+1]\r\n                    p=i+1\r\n    return mmax\r\n\r\n\r\n#\r\ndef lcseque_lens(s1, s2): \r\n     # 生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果\r\n    m = [ [ 0 for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \r\n    # d用来记录转移方向\r\n    d = [ [ None for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \r\n\r\n    for p1 in range(len(s1)): \r\n        for p2 in range(len(s2)): \r\n            if s1[p1] == s2[p2]:            #字符匹配成功，则该位置的值为左上方的值加1\r\n                m[p1+1][p2+1] = m[p1][p2]+1\r\n                d[p1+1][p2+1] = 'ok'          \r\n            elif m[p1+1][p2] > m[p1][p2+1]:  #左值大于上值，则该位置的值为左值，并标记回溯时的方向\r\n                m[p1+1][p2+1] = m[p1+1][p2] \r\n                d[p1+1][p2+1] = 'left'          \r\n            else:                           #上值大于左值，则该位置的值为上值，并标记方向up\r\n                m[p1+1][p2+1] = m[p1][p2+1]   \r\n                d[p1+1][p2+1] = 'up'         \r\n    (p1, p2) = (len(s1), len(s2)) \r\n    s = [] \r\n    while m[p1][p2]:    #不为None时\r\n        c = d[p1][p2]\r\n        if c == 'ok':   #匹配成功，插入该字符，并向左上角找下一个\r\n            s.append(s1[p1-1])\r\n            p1 -= 1\r\n            p2 -= 1 \r\n        if c == 'left':  #根据标记，向左找下一个\r\n            p2 -= 1\r\n        if c == 'up':   #根据标记，向上找下一个\r\n            p1 -= 1\r\n    return len(s)\r\n    \r\ndef compute_cosine(text_a, text_b):\r\n    # 找单词及词频\r\n    words1 = text_a.split(' ')\r\n    words2 = text_b.split(' ')\r\n    # print(words1)\r\n    words1_dict = {}\r\n    words2_dict = {}\r\n    for word in words1:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        # print(word)\r\n        if word != '' and word in words1_dict: # 这里改动了\r\n            num = words1_dict[word]\r\n            words1_dict[word] = num + 1\r\n        elif word != '':\r\n            words1_dict[word] = 1\r\n        else:\r\n            continue\r\n    for word in words2:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        if word != '' and word in words2_dict:\r\n            num = words2_dict[word]\r\n            words2_dict[word] = num + 1\r\n        elif word != '':\r\n            words2_dict[word] = 1\r\n        else:\r\n            continue\r\n\r\n    # 排序\r\n    dic1 = sorted(words1_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n    dic2 = sorted(words2_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n\r\n    # 得到词向量\r\n    words_key = []\r\n    for i in range(len(dic1)):\r\n        words_key.append(dic1[i][0])  # 向数组中添加元素\r\n    for i in range(len(dic2)):\r\n        if dic2[i][0] in words_key:\r\n            # print 'has_key', dic2[i][0]\r\n            pass\r\n        else:  # 合并\r\n            words_key.append(dic2[i][0])\r\n    # print(words_key)\r\n    vect1 = []\r\n    vect2 = []\r\n    for word in words_key:\r\n        if word in words1_dict:\r\n            vect1.append(words1_dict[word])\r\n        else:\r\n            vect1.append(0)\r\n        if word in words2_dict:\r\n            vect2.append(words2_dict[word])\r\n        else:\r\n            vect2.append(0)\r\n\r\n    # 计算余弦相似度\r\n    sum = 0\r\n    sq1 = 0\r\n    sq2 = 0\r\n    for i in range(len(vect1)):\r\n        sum += vect1[i] * vect2[i]\r\n        sq1 += pow(vect1[i], 2)\r\n        sq2 += pow(vect2[i], 2)\r\n    try:\r\n        result = round(float(sum) / (math.sqrt(sq1) * math.sqrt(sq2)), 2)\r\n    except ZeroDivisionError:\r\n        result = 0.0\r\n    return result\r\n\r\ndef Pehrson(text_a, text_b):\r\n    # 找单词及词频\r\n    words1 = text_a.split(' ')\r\n    words2 = text_b.split(' ')\r\n    # print(words1)\r\n    words1_dict = {}\r\n    words2_dict = {}\r\n    for word in words1:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        # print(word)\r\n        if word != '' and word in words1_dict: # 这里改动了\r\n            num = words1_dict[word]\r\n            words1_dict[word] = num + 1\r\n        elif word != '':\r\n            words1_dict[word] = 1\r\n        else:\r\n            continue\r\n    for word in words2:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        if word != '' and word in words2_dict:\r\n            num = words2_dict[word]\r\n            words2_dict[word] = num + 1\r\n        elif word != '':\r\n            words2_dict[word] = 1\r\n        else:\r\n            continue\r\n\r\n    # 排序\r\n    dic1 = sorted(words1_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n    dic2 = sorted(words2_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n\r\n    # 得到词向量\r\n    words_key = []\r\n    for i in range(len(dic1)):\r\n        words_key.append(dic1[i][0])  # 向数组中添加元素\r\n    for i in range(len(dic2)):\r\n        if dic2[i][0] in words_key:\r\n            # print 'has_key', dic2[i][0]\r\n            pass\r\n        else:  # 合并\r\n            words_key.append(dic2[i][0])\r\n    # print(words_key)\r\n    vect1 = []\r\n    vect2 = []\r\n    for word in words_key:\r\n        if word in words1_dict:\r\n            vect1.append(words1_dict[word])\r\n        else:\r\n            vect1.append(0)\r\n        if word in words2_dict:\r\n            vect2.append(words2_dict[word])\r\n        else:\r\n            vect2.append(0)\r\n\r\n    # 计算Pehrson\r\n    x = np.vstack([vect1, vect2])\r\n    return np.corrcoef(x)[0][1]\r\n","execution_count":28},{"metadata":{"id":"2BA57CDA1CF449898BFB071A7F61F048","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"def getfeature(df):\r\n    # df.drop_duplicates(inplace=True)\r\n    since = time.time()\r\n    \r\n    df['query_len'] = df.apply(lambda x: len(x[1].split(' ')), axis=1)\r\n    df['query_len'] = df['query_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def get_query_unique(x):\r\n        a = len(set(x[1].split(' '))-set(x[3].split(' ')))\r\n        return a\r\n    df['query_unique'] = df.apply(get_query_unique, axis=1)\r\n    df['query_unique'] = df['query_unique'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#####################################3\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['title_len'] = df.apply(lambda x: len(x[3].split(' ')), axis=1)\r\n    df['title_len'] = df['title_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def get_query_unique(x):\r\n        a = len(set(x[3].split(' '))-set(x[1].split(' ')))\r\n        return a\r\n    \r\n    df['title_unique'] = df.apply(get_query_unique, axis=1)\r\n    df['title_unique'] = df['title_unique'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['dif'] = (df['query_len'] - df['title_len']).abs()\r\n    df['dif'] = df['dif'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def query_title_same_len(x):\r\n        a = len(x[3].split(' ') and x[1].split(' '))\r\n        return a\r\n    df['query_title_same_len'] = df.apply(query_title_same_len, axis=1)\r\n    df['query_title_same_len'] = df['query_title_same_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n\r\n    def samelen_query_rate(x):\r\n        a = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        return a/len(x[1].split(' '))\r\n    df['samelen_query_rate'] = df.apply(samelen_query_rate, axis=1)\r\n    df['samelen_query_rate'] = df['samelen_query_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def samelen_title_rate(x):\r\n        a = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        return a/len(x[3].split(' '))\r\n    df['samelen_title_rate'] = df.apply(samelen_title_rate, axis=1)\r\n    df['samelen_title_rate'] = df['samelen_title_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    \r\n    def q_t_all_len(x):\r\n        a = len(x[1].split(' ')) + len(x[3].split(' '))\r\n        return a\r\n    df['q_t_all_len'] = df.apply(q_t_all_len, axis=1)\r\n    df['q_t_all_len'] = df['q_t_all_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since##########################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def q_t_all_word_len(x):\r\n        a = len(set(x[1].split(' ')) | set(x[3].split(' ')))\r\n        return a\r\n    df['q_t_all_word_len'] = df.apply(q_t_all_word_len, axis=1)\r\n    df['q_t_all_word_len'] = df['q_t_all_word_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['q_t_diff_len'] = df.apply(lambda x: len(set(x[1].split(' '))^set(x[3].split(' '))), axis=1)\r\n    df['q_t_diff_len'] = df['q_t_diff_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def q_t_diff_q_rate(x):\r\n        a = len(set(x[1].split(' '))^set(x[3].split(' ')))\r\n        return np.float32(a/len(x[1].split(' ')))\r\n    df['q_t_diff_q_rate'] = df.apply(q_t_diff_q_rate, axis=1)\r\n    df['q_t_diff_q_rate'] = df['q_t_diff_q_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def q_t_diff_t_rate(x):\r\n        a = len(set(x[1].split(' '))^set(x[3].split(' ')))\r\n        return np.float32(a/len(x[3].split(' ')))\r\n    df['q_t_diff_t_rate'] = df.apply(q_t_diff_t_rate, axis=1)\r\n    df['q_t_diff_t_rate'] = df['q_t_diff_t_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n\r\n    \r\n    def query_diff_title(x):\r\n        a = len(set(x[1].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.int32(a)\r\n    df['query_diff_title'] = df.apply(query_diff_title, axis=1)\r\n    df['query_diff_title'] = df['query_diff_title'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def query_diff_title_rate(x):\r\n        a = len(set(x[1].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.float32(a/len(x[3].split(' ')))\r\n    df['query_diff_title_rate'] = df.apply(query_diff_title_rate, axis=1)\r\n    df['query_diff_title_rate'] = df['query_diff_title_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def title_diff_query(x):\r\n        a = len(set(x[1].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.int32(a)\r\n    df['title_diff_query'] = df.apply(title_diff_query, axis=1)\r\n    df['title_diff_query'] = df['title_diff_query'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def title_diff_query_rate(x):\r\n        a = len(set(x[1].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.float32(a/len(x[3].split(' ')))\r\n    df['title_diff_query_rate'] = df.apply(title_diff_query_rate, axis=1)\r\n    df['title_diff_query_rate'] = df['title_diff_query_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def dice(x):\r\n        a1 = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        a2 = len(set(x[1].split(' '))) + len(set(x[3].split(' ')))\r\n        return np.float32(2*a1/a2)\r\n    df['dice'] = df.apply(dice, axis=1)\r\n    df['dice'] = df['dice'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    def jaccord(x):\r\n        a1 = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        a2 = len(set(x[1].split(' ')) | set(x[3].split(' ')))\r\n        return np.float32(a1/a2)\r\n    df['jaccord'] = df.apply(jaccord, axis=1)\r\n    df['jaccord'] = df['jaccord'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['query_count'] = df.groupby(1)[1].transform('count')\r\n    df['query_count'] = df['query_count'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['title_count'] = df.groupby(3)[3].transform('count')\r\n    df['title_count'] = df['title_count'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['lcseque_lens'] = df.apply(lambda x: lcseque_lens(x[1], x[3]), axis=1)\r\n    df['lcseque_lens'] = df['lcseque_lens'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['lcsubstr_lens'] = df.apply(lambda x: lcsubstr_lens(x[1], x[3]), axis=1)\r\n    df['lcsubstr_lens'] = df['lcsubstr_lens'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n\r\n    df['ratio'] = df.apply(lambda x: Levenshtein.ratio(x[1], x[3]), axis=1)\r\n    df['ratio'] = df['ratio'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n\r\n    df['jaro'] = df.apply(lambda x: Levenshtein.jaro(x[1], x[3]), axis=1)\r\n    df['jaro'] = df['jaro'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['jaro_winkler'] = df.apply(lambda x: Levenshtein.jaro_winkler(x[1], x[3]), axis=1)\r\n    df['jaro_winkler'] = df['jaro_winkler'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n\r\n    df['cosine_similarity'] = df.apply(lambda x: compute_cosine(x[1], x[3]), axis=1)\r\n    df['cosine_similarity'] = df['cosine_similarity'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n\r\n    df['levenshtein'] = df.apply(lambda x: Levenshtein.distance(x[1], x[3]), axis=1)\r\n    df['levenshtein'] = df['levenshtein'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    \r\n    df['Pehrson'] = df.apply(lambda x:Pehrson(x[1], x[3]), axis=1)\r\n    df['Pehrson'] = df['Pehrson'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n    since = time.time()\r\n    \r\n    df['list_dis'] = df.apply(lambda x: Levenshtein.seqratio(x[1].split(' '), x[3].split(' ')), axis=1)\r\n    df['list_dis'] = df['list_dis'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since\r\n    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\r\n    \r\n\r\n    try:\r\n        df.drop([0, 1, 2, 3, 4], axis=1, inplace=True)\r\n    except:\r\n        df.drop([0, 1, 2, 3], axis=1, inplace=True)\r\n        \r\n    return df\r\n","execution_count":29},{"metadata":{"id":"BA3039D1189244568A265C29CDEFFBC8","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"def augment(x,y,t=2):\r\n    xs,xn = [],[]\r\n    for i in range(t):\r\n        mask = y>0\r\n#        mask = mask.T[0]\r\n        x1 = x[mask].copy()\r\n        ids = np.arange(x1.shape[0])\r\n        for c in range(x1.shape[1]):\r\n            np.random.shuffle(ids)\r\n            x1[:,c] = x1[ids][:,c]\r\n        xs.append(x1)\r\n\r\n    for i in range(t//2):\r\n        mask = y==0\r\n#        mask = mask.T[0]\r\n        x1 = x[mask].copy()\r\n        ids = np.arange(x1.shape[0])\r\n        for c in range(x1.shape[1]):\r\n            np.random.shuffle(ids)\r\n            x1[:,c] = x1[ids][:,c]\r\n        xn.append(x1)\r\n\r\n    xs = np.vstack(xs)\r\n    xn = np.vstack(xn)\r\n    ys = np.ones(xs.shape[0])\r\n#    ys = ys.reshape(ys.shape[0], 1)\r\n    yn = np.zeros(xn.shape[0])\r\n#    yn = yn.reshape(yn.shape[0], 1)\r\n    print(xs.shape)\r\n    print(xn.shape)\r\n    x = np.vstack([x,xs,xn])\r\n    y = np.concatenate([y,ys,yn])\r\n    return x,y\r\n","execution_count":30},{"metadata":{"id":"E6C7AF1A2AFA4AB18D0B21E0D876B463","collapsed":false,"scrolled":false,"hide_input":false},"cell_type":"code","outputs":[],"source":"def vec_for_learning(model, tagged_docs):\r\n    sents = tagged_docs.values\r\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\r\n    return targets, regressors\r\n\r\n# #测试集\r\n# print('测试集')\r\n# # d = pd.DataFrame(test[1] + ' ' +test[3])\r\n# # del test\r\n# test.drop([0, 2], axis=1, inplace=True)\r\n# test['label'] = 0\r\n# test['cut_review'] = test[1].apply(lambda x: [w for w in x.split(' ') if w != ' '])\r\n# test_query_tagged = test.apply(lambda r: TaggedDocument(words=r['cut_review'], tags=[r['label']]), axis=1)\r\n# test['cut_review'] = test[3].apply(lambda x: [w for w in x.split(' ') if w != ' '])\r\n# test_title_tagged = test.apply(lambda r: TaggedDocument(words=r['cut_review'], tags=[r['label']]), axis=1)\r\n# del test\r\n# print('测试集分词完成')\r\n\r\n# kf_n = 5\r\n# kf = KFold(n_splits=kf_n, shuffle=True, random_state=2)\r\n\r\n# pre_lr = 0\r\n# pre_lgb = 0\r\n\r\n# lr = LogisticRegression(penalty='l1', C=0.14, n_jobs=4)\r\n\r\nx = 1\r\nfor df in train:\r\n    if x != 10:\r\n        print(x)\r\n        x += 1\r\n        continue\r\n    \r\n    print('第%d次处理开始'%x)\r\n    \r\n    test[4] = 0\r\n    print('合并训练集和测试集')\r\n    len_train = len(df)\r\n    df = pd.concat((df, test))\r\n    del test\r\n\r\n\r\n    #训练集\r\n    # n_train = np.random.choice(range(len(df)), 5000000, replace=False)\r\n    # df_train = df.iloc[n_train, :]\r\n#    df['all_review'] = pd.DataFrame(df[1] + ' ' +df[3])\r\n    # d = pd.DataFrame(df_train[1] + ' ' +df_train[3])\r\n    # d['label'] = df_train[4]\r\n    df['cut_review'] = df[1].apply(lambda x: [w for w in x.split(' ') if w != ' '])\r\n    train_query_tagged = df.apply(lambda r: TaggedDocument(words=r['cut_review'], tags=[r[4]]), axis=1)\r\n    df['cut_review'] = df[3].apply(lambda x: [w for w in x.split(' ') if w != ' '])\r\n    train_title_tagged = df.apply(lambda r: TaggedDocument(words=r['cut_review'], tags=[r[4]]), axis=1)\r\n\r\n    del df\r\n\r\n    # #验证集\r\n    # n_vail = np.random.choice(range(len(df)), 1000000, replace=False)\r\n    # df_vail = df.iloc[n_vail, :]\r\n    # d = pd.DataFrame(df_vail[1] + ' ' +df_vail[3])\r\n    # d['label'] = df_vail[4]\r\n    # d['cut_review'] = d[0].apply(lambda x: [w for w in list(jieba.cut(x)) if w != ' '])\r\n    # vail_tagged = d.apply(lambda r: TaggedDocument(words=r['cut_review'], tags=[r['label']]), axis=1)\r\n    \r\n    # del d\r\n    # del n_vail\r\n    # del df_vail\r\n    # del df\r\n    \r\n\r\n    # #测试集\r\n    # d = pd.DataFrame(test[1] + ' ' +test[3])\r\n    # d['label'] = 0\r\n    # d['cut_review'] = d[0].apply(lambda x: [w for w in list(jieba.cut(x)) if w != ' '])\r\n    # test_tagged = d.apply(lambda r: TaggedDocument(words=r['cut_review'], tags=[r['label']]), axis=1)\r\n\r\n    \r\n    model_dbow = Doc2Vec(dm=0,  negative=5, hs=0, min_count=100, sample = 0, workers=2)\r\n    model_dbow.build_vocab([x for x in tqdm(train_query_tagged.values)])\r\n\r\n    for epoch in range(5):\r\n        model_dbow.train(utils.shuffle([x for x in tqdm(train_query_tagged.values)]), \r\n                                total_examples=len(train_query_tagged.values), epochs=2)\r\n        model_dbow.alpha -= 0.002\r\n        model_dbow.min_alpha = model_dbow.alpha\r\n        \r\n    \r\n    print('第%d次处理完成训练开始'%x)\r\n    \r\n    #训练集向量化\r\n    print('query训练')\r\n    y_train, x_train = vec_for_learning(model_dbow, train_query_tagged)\r\n    del train_query_tagged\r\n    x_train = np.array(x_train)\r\n    y_train = np.array(y_train)\r\n    # x_train, y_train = augment(x_train, y_train)\r\n    # x_train = np.hstack((x_train, df_train.iloc[:lentrain, :]))\r\n    print(x_train[:len_train].shape, x_train[len_train:].shape)\r\n    np.save('train_query_doc2vec_last1000W.npy', x_train[:len_train])\r\n    np.save('train_label_doc2vec_last1000W.npy', y_train[:len_train])\r\n    np.save('test_query_doc2vec_last1000W.npy', x_train[len_train:])\r\n    del x_train\r\n    del y_train\r\n    \r\n\r\n    # #验证集向量化\r\n    # y_vail, x_vail = vec_for_learning(model_dbow, vail_tagged)\r\n    # del vail_tagged\r\n    # x_vail = np.array(x_vail)\r\n    # y_vail = np.array(y_vail)\r\n    # print(x_vail.shape)\r\n    \r\n    #测试集向量化\r\n    # _, x_test = vec_for_learning(model_dbow, test_query_tagged)\r\n    # del test_query_tagged\r\n    # x_test = np.array(x_test)\r\n    # # x_test = np.hstack((x_test, df_train.iloc[lentrain:, :]))\r\n    # print(x_test.shape)\r\n    # np.save('test_query_doc2vec_last1000W.npy', x_test)\r\n    # del x_test\r\n    \r\n    \r\n    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\r\n    del model_dbow\r\n\r\n    print('title训练')\r\n    model_dbow = Doc2Vec(dm=0,  negative=5, hs=0, min_count=100, sample = 0, workers=2)\r\n    model_dbow.build_vocab([x for x in tqdm(train_title_tagged.values)])\r\n\r\n    for epoch in range(5):\r\n        model_dbow.train(utils.shuffle([x for x in tqdm(train_title_tagged.values)]), \r\n                                total_examples=len(train_title_tagged.values), epochs=2)\r\n        model_dbow.alpha -= 0.002\r\n        model_dbow.min_alpha = model_dbow.alpha\r\n        \r\n    \r\n    y_train, x_train = vec_for_learning(model_dbow, train_title_tagged)\r\n    del train_title_tagged\r\n    x_train = np.array(x_train)\r\n    # x_train, y_train = augment(x_train, y_train)\r\n    # x_train = np.hstack((x_train, df_train.iloc[:lentrain, :]))\r\n    print(x_train.shape)\r\n    np.save('train_title_doc2vec_last1000W.npy', x_train[:len_train])\r\n    np.save('test_title_doc2vec_last1000W.npy', x_train[len_train:])\r\n    del x_train\r\n    del y_train\r\n    \r\n    # #测试集向量化\r\n    # _, x_test = vec_for_learning(model_dbow, test_title_tagged)\r\n    # del test_title_tagged\r\n    # x_test = np.array(x_test)\r\n    # # x_test = np.hstack((x_test, df_train.iloc[lentrain:, :]))\r\n    # print(x_test.shape)\r\n    # np.save('test_title_doc2vec_last1000W.npy', x_test)\r\n    # del x_test\r\n    \r\n    print('结束')\r\n    break\r\n    \r\n    \r\n    ","execution_count":31},{"metadata":{"id":"346DA654633B46AAA9639F43032F47C4","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"# kf_n = 5\r\n# kf = KFold(n_splits=kf_n, shuffle=True)\r\n\r\n# pre_lr = 0\r\n\r\n# lr = LogisticRegression(penalty='l2', C=0.3, n_jobs=4)\r\n\r\n# for x_tr, x_vail in kf.split(x_train):\r\n        \r\n#         lr.fit(x_train[x_tr], y_train[x_tr])\r\n#         print(metrics.roc_auc_score(y_train[x_vail], lr.predict_proba(x_train[x_vail])[:, 1]))\r\n#         print(metrics.log_loss(y_train[x_vail], lr.predict_proba(x_train[x_vail])[:, 1]))\r\n#         pre_lr += lr.predict_proba(x_test)[:, 1]\r\n\r\n# pre_lr /= kf_n\r\n\r\n# pre_df = pre_df.rename(columns={0:'query_id', 2:'query_title_id'})\r\n# pre_df['prediction'] = pre_lr\r\n# pre_df.to_csv('lr_doc2vec_last500w.csv', index=False, header=False)","execution_count":32},{"metadata":{"id":"C05348C7C0604028B5094E8813B6CB76","collapsed":false,"scrolled":false,"hide_input":true},"cell_type":"code","outputs":[],"source":"# pre_lgb = 0\r\n\r\n# random_state = 2\r\n\r\n# # lgb_params = {\r\n# #     \"objective\" : \"binary\",\r\n# #     # \"metric\" : \"auc\",\r\n# #     \"boosting\": 'gbdt',\r\n# #     \"max_depth\" : -1,\r\n# #     \"num_leaves\" : 5,\r\n# #     \"learning_rate\" : 0.1,\r\n# #     \"bagging_freq\": 5,\r\n# #     \"bagging_fraction\" : 0.6,\r\n# #     \"feature_fraction\" : 0.05,\r\n# #     \"min_data_in_leaf\": 700,\r\n# #     \"min_sum_heassian_in_leaf\": 10,\r\n# #     \"tree_learner\": \"serial\",\r\n# #     \"boost_from_average\": \"false\",\r\n# # #    \"lambda_l1\" : 5,\r\n# # #    \"lambda_l2\" : 5,\r\n# #     \"bagging_seed\" : random_state,\r\n# #     \"verbosity\" : 1,\r\n# #     \"seed\": random_state\r\n# # }\r\n\r\n# lgb_params = {\r\n#     'boosting_type': 'gbdt',\r\n#     'objective': \"binary\",\r\n#     'metric': ['binary_logloss',\"auc\"],\r\n#     'num_leaves': 32,\r\n#     'learning_rate': 0.05,\r\n#     'feature_fraction': 0.9,\r\n#     'bagging_fraction': 0.8,\r\n#     'bagging_freq': 5,\r\n#     'verbose': 1\r\n# }\r\n\r\n\r\n# for x_tr, x_vail in kf.split(x_train):\r\n        \r\n#         #当x_train为ndarray时，下列数据集的划分会进行复制而占用内存\r\n        \r\n#         # x_t1 = x_train[x_tr]\r\n#         # y_t1 = y_train[x_tr]\r\n        \r\n#         # x_t2 = x_train[x_vail]\r\n#         # y_t2 = y_train[x_vail]\r\n            \r\n#         lgbtr1=lgb.Dataset(x_train[x_tr], y_train[x_tr])\r\n#         lgbtr2=lgb.Dataset(x_train[x_vail], y_train[x_vail])\r\n            \r\n#         evals_result = {}\r\n#         gbm = lgb.train(lgb_params,\r\n#                           lgbtr1,\r\n#                           num_boost_round=1000,\r\n#                           valid_sets=lgbtr2,\r\n#                           verbose_eval=50,\r\n#                           early_stopping_rounds=50,\r\n#                           evals_result=evals_result)\r\n                          \r\n#         print(metrics.roc_auc_score(y_train[x_vail], gbm.predict(x_train[x_vail])))\r\n#         pre_lgb += gbm.predict(x_test, num_iteration=gbm.best_iteration)\r\n        \r\n# pre_lgb /= kr_n\r\n\r\n# pre_df = pre_df.rename(columns={0:'query_id', 2:'query_title_id'})\r\n# pre_df['prediction'] = pre_lgb\r\n# pre_df.to_csv('lgb_doc2vec_last500w.csv', index=False, header=False)\r\n","execution_count":33},{"metadata":{"id":"DB6EAA860B324761AD27A2E0DAB30B12","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# !./kesci_submit -token 5ebea4ff5f5fa8e0 -file lgb_doc2vec_last500w.csv","execution_count":34}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}