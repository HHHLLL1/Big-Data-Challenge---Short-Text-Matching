{"cells":[{"outputs":[{"output_type":"stream","text":"bytedance\r\n","name":"stdout"}],"execution_count":1,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"3B66FD8EC5174F6AAF4434AAE9C167E2","scrolled":false}},{"outputs":[{"output_type":"stream","text":"feature_label_First1ELast1E_test.txt  query_titleConcat_label.txt\r\nlost+found\t\t\t      query_titleConcat.txt\r\npre_csv.txt\r\n","name":"stdout"}],"execution_count":2,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"0BACE7A862224CF8B97D502F8E847A71","scrolled":false}},{"outputs":[{"output_type":"stream","text":"Package                       Version    \n----------------------------- -----------\nabsl-py                       0.7.1      \nalabaster                     0.7.12     \nallennlp                      0.8.0      \naltair                        1.2.0      \narrow                         0.12.1     \nasn1crypto                    0.24.0     \nastor                         0.8.0      \natomicwrites                  1.3.0      \nattrs                         19.1.0     \naudioread                     2.1.8      \naws-xray-sdk                  0.95       \nawscli                        1.16.193   \nBabel                         2.7.0      \nbackcall                      0.1.0      \nbeautifulsoup4                4.7.1      \nblaze                         0.10.1     \nbleach                        3.1.0      \nblinker                       1.4        \nbokeh                         1.0.2      \nboto                          2.49.0     \nboto3                         1.9.183    \nbotocore                      1.12.183   \nbs4                           0.0.1      \nbunch                         1.0.1      \ncatboost                      0.8.1      \ncertifi                       2018.1.18  \ncffi                          1.11.5     \nchardet                       3.0.4      \nClick                         7.0        \ncloudpickle                   1.2.1      \ncolorama                      0.3.9      \ncolorlover                    0.3.0      \nconda                         4.4.10     \nconfigparser                  3.7.4      \nconllu                        0.11       \ncookies                       2.2.1      \ncryptography                  2.1.4      \ncufflinks                     0.12.1     \ncycler                        0.10.0     \ncymem                         2.0.2      \nCython                        0.29.12    \ncytoolz                       0.9.0.1    \ndask                          2.0.0      \ndatashape                     0.5.2      \ndeap                          1.3.0      \ndecorator                     4.4.0      \ndefusedxml                    0.6.0      \nDelorean                      0.6.0      \ndill                          0.2.9      \ndocker                        4.0.2      \ndocopt                        0.6.2      \ndocutils                      0.14       \necdsa                         0.13.2     \neditdistance                  0.5.3      \nentrypoints                   0.3        \nfbprophet                     0.3.post2  \nfitter                        1.0.8      \nflaky                         3.6.0      \nFlask                         1.0.2      \nFlask-Cors                    3.0.7      \nftfy                          5.5.1      \nfuncy                         1.12       \nfuture                        0.17.1     \ngast                          0.2.2      \ngensim                        3.4.0      \nGeohash                       1.0        \ngevent                        1.3.6      \ngpxpy                         1.1.2      \ngraphviz                      0.8.4      \ngreenlet                      0.4.15     \ngrpcio                        1.22.0     \nh2o                           3.18.0.4   \nh5py                          2.8.0rc1   \nhaversine                     0.4.5      \nheamy                         0.0.7      \nhmmlearn                      0.2.1      \nhumanize                      0.5.1      \nidna                          2.6        \nimagesize                     1.1.0      \nimportlib-metadata            0.18       \nipykernel                     5.1.1      \nipython                       7.6.1      \nipython-genutils              0.2.0      \nipywidgets                    7.5.0      \nitsdangerous                  1.1.0      \njedi                          0.14.0     \njieba                         0.39       \nJinja2                        2.10.1     \njmespath                      0.9.4      \njoblib                        0.13.2     \njsondiff                      1.1.1      \njsonnet                       0.10.0     \njsonpickle                    1.2        \njsonschema                    3.0.1      \njupyter                       1.0.0      \njupyter-client                4.4.0      \njupyter-console               6.0.0      \njupyter-core                  4.5.0      \njupyter-kernel-gateway        1.2.0      \nKeras                         2.2.4      \nKeras-Applications            1.0.8      \nKeras-Preprocessing           1.1.0      \nkiwisolver                    1.1.0      \nklab-autotime                 0.0.2      \nlangid                        1.1.6      \nlibrosa                       0.6.1      \nlightgbm                      2.2.4      \nline-profiler                 2.1.2      \nllvmlite                      0.29.0     \nlxml                          4.2.1      \nMarkdown                      3.1.1      \nMarkupSafe                    1.1.1      \nmatplotlib                    3.1.1      \nmatplotlib-venn               0.11.5     \nMDP                           3.5        \nmissingno                     0.4.0      \nmistune                       0.8.4      \nml-metrics                    0.1.4      \nmlxtend                       0.12.0     \nmock                          3.0.5      \nmore-itertools                7.1.0      \nmoto                          1.3.4      \nmpld3                         0.3        \nmplleaflet                    0.0.5      \nmpmath                        1.1.0      \nmsgpack                       0.5.6      \nmsgpack-numpy                 0.4.3.2    \nmultipledispatch              0.6.0      \nmurmurhash                    1.0.2      \nmxnet                         1.1.0.post0\nnbconvert                     5.5.0      \nnbformat                      4.4.0      \nnetaddr                       0.7.19     \nnetworkx                      2.1        \nnibabel                       2.4.1      \nNiftyNet                      0.3.0      \nnltk                          3.3        \nnotebook                      4.4.1      \nnumba                         0.44.1     \nnumexpr                       2.6.9      \nnumpy                         1.15.4     \nnumpydoc                      0.8.0      \nodo                           0.5.0      \nonnx                          1.3.0      \nopencv-python                 3.4.4.19   \norderedmultidict              1.0        \noverrides                     1.9        \npackaging                     19.0       \npandas                        0.23.4     \npandas-profiling              1.4.0      \npandocfilters                 1.4.2      \nparsimonious                  0.8.0      \nparso                         0.5.0      \npatsy                         0.5.1      \npexpect                       4.7.0      \npickleshare                   0.7.5      \nPillow                        5.3.0      \npip                           9.0.1      \nplac                          0.9.6      \nplotly                        3.4.2      \npluggy                        0.12.0     \npreshed                       2.0.1      \nprometheus-client             0.7.1      \nprompt-toolkit                2.0.9      \nprotobuf                      3.8.0      \npsutil                        5.6.3      \nptyprocess                    0.6.0      \npudb                          2017.1     \npy                            1.8.0      \npyaml                         19.4.1     \npyasn1                        0.4.5      \npycosat                       0.6.3      \npycparser                     2.18       \npycryptodome                  3.8.2      \npydot                         1.2.3      \npygal                         2.4.0      \nPygments                      2.4.2      \npyLDAvis                      2.1.1      \npyOpenSSL                     17.5.0     \npyparsing                     2.1.10     \npyrsistent                    0.15.3     \nPySocks                       1.6.7      \npystan                        2.18.0.0   \npytest                        5.0.1      \npython-dateutil               2.8.0      \npython-jose                   2.0.2      \npython-Levenshtein            0.12.0     \npython-speech-features        0.6        \npytorch-pretrained-bert       0.3.0      \npytz                          2017.3     \nPyWavelets                    1.0.3      \nPyYAML                        5.1        \npyzmq                         18.0.2     \nqtconsole                     4.5.1      \nregex                         2018.1.10  \nrequests                      2.18.4     \nresampy                       0.2.1      \nresponses                     0.10.6     \nretrying                      1.3.3      \nrsa                           3.4.2      \nruamel-yaml                   0.15.35    \ns2sphere                      0.2.4      \ns3transfer                    0.2.1      \nsacred                        0.6.10     \nscikit-image                  0.14.1     \nscikit-learn                  0.20.2     \nscipy                         1.2.0      \nseaborn                       0.9.0      \nSend2Trash                    1.5.0      \nsetuptools                    38.4.0     \nSexMachine                    0.1.1      \nsix                           1.11.0     \nsmart-open                    1.8.4      \nsmhasher                      0.150.1    \nsnowballstemmer               1.9.0      \nsoupsieve                     1.9.2      \nspacy                         2.0.18     \nSphinx                        2.1.2      \nsphinxcontrib-applehelp       1.0.1      \nsphinxcontrib-devhelp         1.0.1      \nsphinxcontrib-htmlhelp        1.0.2      \nsphinxcontrib-jsmath          1.0.1      \nsphinxcontrib-qthelp          1.0.2      \nsphinxcontrib-serializinghtml 1.1.3      \nSQLAlchemy                    1.3.5      \nsqlparse                      0.2.4      \nstatsmodels                   0.9.0      \nsympy                         1.2        \ntables                        3.4.2      \ntabulate                      0.8.3      \ntensorboard                   1.12.2     \ntensorboardX                  1.2        \ntensorflow-gpu                1.12.0     \ntermcolor                     1.1.0      \nterminado                     0.8.2      \ntestpath                      0.4.2      \ntextblob                      0.15.1     \nTheano                        1.0.3      \nthinc                         6.12.1     \ntoolz                         0.8.2      \ntorch                         1.1.0      \ntorchvision                   0.3.0      \ntornado                       4.5.3      \nTPOT                          0.6.8      \ntqdm                          4.32.2     \ntraitlets                     4.3.2      \ntrueskill                     0.4.4      \ntyping                        3.7.4      \ntyping-extensions             3.7.4      \ntzlocal                       1.5.1      \nujson                         1.35       \nUnidecode                     1.1.1      \nupdate-checker                0.16       \nurllib3                       1.22       \nurwid                         2.0.1      \nvega                          2.4.0      \nvida                          0.3        \nwcwidth                       0.1.7      \nwebencodings                  0.5.1      \nwebsocket-client              0.56.0     \nWerkzeug                      0.15.4     \nwheel                         0.30.0     \nwidgetsnbextension            3.5.0      \nwordcloud                     1.4.1      \nwrapt                         1.10.11    \nxgboost                       0.81       \nxlearn                        0.40a1     \nxlrd                          1.0.0      \nxmltodict                     0.12.0     \nzipp                          0.5.2      \n","name":"stdout"}],"execution_count":3,"source":"# 查看当前kernerl下的package\n# !pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":true,"id":"BC5A1238454B4764B01AC5E1DF5CDA82","scrolled":false}},{"outputs":[],"execution_count":4,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"E874620CE8A7466B8B4FB105157AF5AE","scrolled":false}},{"metadata":{"id":"942327975B594B9BA42C572BBC82B9C4","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 941 ms\n","name":"stdout"}],"source":"import numpy as np\r\nimport pandas as pd\r\nimport Levenshtein\r\nimport math\r\nimport time\r\nfrom gensim.models import Word2Vec, Doc2Vec\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\r\nimport lightgbm as lgb\r\nimport multiprocessing\r\nimport gc","execution_count":5},{"metadata":{"id":"68CE6B5F3A6B498BA6008B2246A5593B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"7"},"execution_count":7},{"output_type":"stream","text":"time: 1min 38s\n","name":"stdout"}],"source":"path=\"/home/kesci/input/bytedance/\"\r\ntrain = pd.read_csv(path+'train_final.csv', header=None, chunksize=100000000, names=['query_id', 'query', 'title_id', 'title', 'label'])\r\ntest = pd.read_csv(path+'test_final_part1.csv', header=None, names=['query_id', 'query', 'title_id', 'title'])\r\npre_csv = test[['query_id', 'title_id']]\r\npre_csv.to_csv('pre_csv.txt', index=False)\r\ndel pre_csv\r\ngc.collect()","execution_count":7},{"metadata":{"id":"287869638DD4448DBBACE65EEE425173","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 9.45 s\n","name":"stdout"}],"source":"#去除'\\t'符号\ntest['title'] = test['title'].apply(lambda x: x[:-1]if x[-1] == '\\t' else x)","execution_count":8},{"metadata":{"id":"716AF8FF628649809C40A221A17983FA","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 58.3 ms\n","name":"stdout"}],"source":"def lcsubstr_lens(s1, s2): \r\n    m=[[0 for i in range(len(s2)+1)]  for j in range(len(s1)+1)]  #生成0矩阵，为方便后续计算，比字符串长度多了一列\r\n    mmax=0   #最长匹配的长度\r\n    p=0  #最长匹配对应在s1中的最后一位\r\n    for i in range(len(s1)):\r\n        for j in range(len(s2)):\r\n            if s1[i]==s2[j]:\r\n                m[i+1][j+1]=m[i][j]+1\r\n                if m[i+1][j+1]>mmax:\r\n                    mmax=m[i+1][j+1]\r\n                    p=i+1\r\n    return mmax\r\n\r\n\r\n#\r\ndef lcseque_lens(s1, s2): \r\n     # 生成字符串长度加1的0矩阵，m用来保存对应位置匹配的结果\r\n    m = [ [ 0 for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \r\n    # d用来记录转移方向\r\n    d = [ [ None for x in range(len(s2)+1) ] for y in range(len(s1)+1) ] \r\n\r\n    for p1 in range(len(s1)): \r\n        for p2 in range(len(s2)): \r\n            if s1[p1] == s2[p2]:            #字符匹配成功，则该位置的值为左上方的值加1\r\n                m[p1+1][p2+1] = m[p1][p2]+1\r\n                d[p1+1][p2+1] = 'ok'          \r\n            elif m[p1+1][p2] > m[p1][p2+1]:  #左值大于上值，则该位置的值为左值，并标记回溯时的方向\r\n                m[p1+1][p2+1] = m[p1+1][p2] \r\n                d[p1+1][p2+1] = 'left'          \r\n            else:                           #上值大于左值，则该位置的值为上值，并标记方向up\r\n                m[p1+1][p2+1] = m[p1][p2+1]   \r\n                d[p1+1][p2+1] = 'up'         \r\n    (p1, p2) = (len(s1), len(s2)) \r\n    s = [] \r\n    while m[p1][p2]:    #不为None时\r\n        c = d[p1][p2]\r\n        if c == 'ok':   #匹配成功，插入该字符，并向左上角找下一个\r\n            s.append(s1[p1-1])\r\n            p1 -= 1\r\n            p2 -= 1 \r\n        if c == 'left':  #根据标记，向左找下一个\r\n            p2 -= 1\r\n        if c == 'up':   #根据标记，向上找下一个\r\n            p1 -= 1\r\n    return len(s)\r\n\r\n\r\ndef levenshtein(first, second):\r\n        ''' 编辑距离算法（LevD） \r\n            Args: 两个字符串\r\n            returns: 两个字符串的编辑距离 int\r\n        '''\r\n        if len(first) > len(second):\r\n            first, second = second, first\r\n        if len(first) == 0:\r\n            return len(second)\r\n        if len(second) == 0:\r\n            return len(first)\r\n        first_length = len(first) + 1\r\n        second_length = len(second) + 1\r\n        distance_matrix = [list(range(second_length)) for x in range(first_length)]\r\n        # print distance_matrix\r\n        for i in range(1, first_length):\r\n            for j in range(1, second_length):\r\n                deletion = distance_matrix[i - 1][j] + 1\r\n                insertion = distance_matrix[i][j - 1] + 1\r\n                substitution = distance_matrix[i - 1][j - 1]\r\n                if first[i - 1] != second[j - 1]:\r\n                    substitution += 1\r\n                distance_matrix[i][j] = min(insertion, deletion, substitution)\r\n                # print distance_matrix\r\n        return distance_matrix[first_length - 1][second_length - 1]\r\n    \r\n    \r\n#ratio\r\ndef ratio(first, second):\r\n        ''' 编辑距离算法（LevD） \r\n            Args: 两个字符串\r\n            returns: 两个字符串的编辑距离 int\r\n        '''\r\n        if len(first) > len(second):\r\n            first, second = second, first\r\n        if len(first) == 0:\r\n            return len(second)\r\n        if len(second) == 0:\r\n            return len(first)\r\n        first_length = len(first) + 1\r\n        second_length = len(second) + 1\r\n        sum_len = first_length + second_length \r\n        distance_matrix = [list(range(second_length)) for x in range(first_length)]\r\n        # print distance_matrix\r\n        for i in range(1, first_length):\r\n            for j in range(1, second_length):\r\n                deletion = distance_matrix[i - 1][j] + 1\r\n                insertion = distance_matrix[i][j - 1] + 1\r\n                substitution = distance_matrix[i - 1][j - 1]\r\n                if first[i - 1] != second[j - 1]:\r\n                    substitution += 2\r\n                distance_matrix[i][j] = min(insertion, deletion, substitution)\r\n                # print distance_matrix\r\n        return np.float32((sum_len - distance_matrix[first_length - 1][second_length - 1])/sum_len)\r\n   \r\n#余弦相似度\r\ndef compute_cosine(text_a, text_b):\r\n    # 找单词及词频\r\n    words1 = text_a.split(' ')\r\n    words2 = text_b.split(' ')\r\n    # print(words1)\r\n    words1_dict = {}\r\n    words2_dict = {}\r\n    for word in words1:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        # print(word)\r\n        if word != '' and word in words1_dict: # 这里改动了\r\n            num = words1_dict[word]\r\n            words1_dict[word] = num + 1\r\n        elif word != '':\r\n            words1_dict[word] = 1\r\n        else:\r\n            continue\r\n    for word in words2:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        if word != '' and word in words2_dict:\r\n            num = words2_dict[word]\r\n            words2_dict[word] = num + 1\r\n        elif word != '':\r\n            words2_dict[word] = 1\r\n        else:\r\n            continue\r\n\r\n    # 排序\r\n    dic1 = sorted(words1_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n    dic2 = sorted(words2_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n\r\n    # 得到词向量\r\n    words_key = []\r\n    for i in range(len(dic1)):\r\n        words_key.append(dic1[i][0])  # 向数组中添加元素\r\n    for i in range(len(dic2)):\r\n        if dic2[i][0] in words_key:\r\n            # print 'has_key', dic2[i][0]\r\n            pass\r\n        else:  # 合并\r\n            words_key.append(dic2[i][0])\r\n    # print(words_key)\r\n    vect1 = []\r\n    vect2 = []\r\n    for word in words_key:\r\n        if word in words1_dict:\r\n            vect1.append(words1_dict[word])\r\n        else:\r\n            vect1.append(0)\r\n        if word in words2_dict:\r\n            vect2.append(words2_dict[word])\r\n        else:\r\n            vect2.append(0)\r\n\r\n    # 计算余弦相似度\r\n    sum = 0\r\n    sq1 = 0\r\n    sq2 = 0\r\n    for i in range(len(vect1)):\r\n        sum += vect1[i] * vect2[i]\r\n        sq1 += pow(vect1[i], 2)\r\n        sq2 += pow(vect2[i], 2)\r\n    try:\r\n        result = round(float(sum) / (math.sqrt(sq1) * math.sqrt(sq2)), 2)\r\n    except ZeroDivisionError:\r\n        result = 0.0\r\n    return result\r\n\r\n\r\n\r\n#皮尔逊系数\r\ndef Pehrson(text_a, text_b):\r\n    # 找单词及词频\r\n    words1 = text_a.split(' ')\r\n    words2 = text_b.split(' ')\r\n    # print(words1)\r\n    words1_dict = {}\r\n    words2_dict = {}\r\n    for word in words1:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        # print(word)\r\n        if word != '' and word in words1_dict: # 这里改动了\r\n            num = words1_dict[word]\r\n            words1_dict[word] = num + 1\r\n        elif word != '':\r\n            words1_dict[word] = 1\r\n        else:\r\n            continue\r\n    for word in words2:\r\n        # word = word.strip(\",.?!;\")\r\n        word = word.lower()\r\n        if word != '' and word in words2_dict:\r\n            num = words2_dict[word]\r\n            words2_dict[word] = num + 1\r\n        elif word != '':\r\n            words2_dict[word] = 1\r\n        else:\r\n            continue\r\n\r\n    # 排序\r\n    dic1 = sorted(words1_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n    dic2 = sorted(words2_dict.items(), key=lambda asd: asd[1], reverse=True)\r\n\r\n    # 得到词向量\r\n    words_key = []\r\n    for i in range(len(dic1)):\r\n        words_key.append(dic1[i][0])  # 向数组中添加元素\r\n    for i in range(len(dic2)):\r\n        if dic2[i][0] in words_key:\r\n            # print 'has_key', dic2[i][0]\r\n            pass\r\n        else:  # 合并\r\n            words_key.append(dic2[i][0])\r\n    # print(words_key)\r\n    vect1 = []\r\n    vect2 = []\r\n    for word in words_key:\r\n        if word in words1_dict:\r\n            vect1.append(words1_dict[word])\r\n        else:\r\n            vect1.append(0)\r\n        if word in words2_dict:\r\n            vect2.append(words2_dict[word])\r\n        else:\r\n            vect2.append(0)\r\n\r\n    # 计算Pehrson\r\n    x = np.vstack([vect1, vect2])\r\n    return np.corrcoef(x)[0][1]\r\n\r\n\r\n\r\n#\r\ndef getBayesSmoothParam(origion_rate):\r\n    origion_rate_mean = origion_rate.mean()\r\n    origion_rate_var = origion_rate.var()\r\n    alpha = origion_rate_mean / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\r\n    beta = (1 - origion_rate_mean) / origion_rate_var * (origion_rate_mean * (1 - origion_rate_mean) - origion_rate_var)\r\n    return alpha, beta\r\n\r\n\r\n# 统计单维度的转化率特征\r\ndef get_single_dimension_rate_train_feature(train_df, fea_set):\r\n    skf = StratifiedKFold(n_splits=5, random_state=2019, shuffle=True)\r\n    for fea in fea_set:\r\n        train_temp_df = pd.DataFrame()\r\n        for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\r\n            temp_df = train_df[[fea, 'label']].iloc[train_index].copy()\r\n            temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\r\n            temp_pivot_table.reset_index(inplace=True)\r\n            temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\r\n            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\r\n            temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\r\n            fea_df = train_df.iloc[test_index].copy()\r\n            fea_df = pd.merge(fea_df, temp_pivot_table, on=fea, how='left')\r\n            train_temp_df = pd.concat([train_temp_df, fea_df])\r\n        print(fea + ' : finish!!!')\r\n        train_df = train_temp_df\r\n        train_df.sort_index(by='index', ascending=True, inplace=True)\r\n    return train_df\r\n\r\n\r\n\r\n# 统计双维度交叉转化率\r\ndef get_jiaocha_dimension_rate_train_feature(train_df, fea_set):\r\n    skf = StratifiedKFold(n_splits=5, random_state=2019, shuffle=True)\r\n    for i in range(len(fea_set)):\r\n        for j in range((i+1), len(fea_set)):\r\n            fea1 = fea_set[i]\r\n            fea2 = fea_set[j]\r\n            train_temp_df = pd.DataFrame()\r\n            for index, (train_index, test_index) in enumerate(skf.split(train_df, train_df['label'])):\r\n                temp_df = train_df[[fea1, fea2, 'label']].iloc[train_index].copy()\r\n                temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\r\n                temp_pivot_table.reset_index(inplace=True)\r\n                temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\r\n                alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\r\n                temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\r\n                fea_df = train_df.iloc[test_index].copy()\r\n                fea_df = pd.merge(fea_df, temp_pivot_table, on=[fea1, fea2], how='left')\r\n                train_temp_df = pd.concat([train_temp_df, fea_df])\r\n            print(fea1 + '_' + fea2 + ' : finish!!!')\r\n            train_df = train_temp_df\r\n            train_df.sort_index(by='index', ascending=True, inplace=True)\r\n    return train_df\r\n\r\n\r\n# 统计单维度的转化率特征\r\ndef get_single_dimension_rate_test_feature(train_df, valid_df, fea_set):\r\n    for fea in fea_set:\r\n        temp_df = train_df[[fea, 'label']].copy()\r\n        temp_pivot_table = pd.pivot_table(temp_df, index=fea, values='label', aggfunc={len, np.mean, np.sum})\r\n        temp_pivot_table.reset_index(inplace=True)\r\n        temp_pivot_table.rename(columns={'len':fea + '_count', 'mean':fea + '_rate', 'sum':fea + '_click_number'}, inplace=True)\r\n        alpha, beta = getBayesSmoothParam(temp_pivot_table[fea + '_rate'])\r\n        temp_pivot_table[fea + '_rate'] = (temp_pivot_table[fea + '_click_number'] + alpha) / (temp_pivot_table[fea + '_count'] + alpha + beta)\r\n        valid_df = pd.merge(valid_df, temp_pivot_table, on=fea, how='left')\r\n        print(fea + ' : finish!!!')\r\n    return valid_df\r\n\r\n# 统计双维度交叉转化率\r\ndef get_jiaocha_dimension_rate_test_feature(train_df, valid_df, fea_set):\r\n    for i in range(len(fea_set)):\r\n        for j in range((i+1), len(fea_set)):\r\n            fea1 = fea_set[i]\r\n            fea2 = fea_set[j]\r\n            temp_df = train_df[[fea1, fea2, 'label']].copy()\r\n            temp_pivot_table = pd.pivot_table(temp_df, index=[fea1, fea2], values='label', aggfunc={len, np.mean, np.sum})\r\n            temp_pivot_table.reset_index(inplace=True)\r\n            temp_pivot_table.rename(columns={'len':fea1 + '_' + fea2 + '_count', 'mean':fea1 + '_' + fea2 + '_rate', 'sum':fea1 + '_' + fea2 + '_click_number'}, inplace=True)\r\n            alpha, beta = getBayesSmoothParam(temp_pivot_table[fea1 + '_' + fea2 + '_rate'])\r\n            temp_pivot_table[fea1 + '_' + fea2 + '_rate'] = (temp_pivot_table[fea1 + '_' + fea2 + '_click_number'] + alpha) / (temp_pivot_table[fea1 + '_' + fea2 + '_count'] + alpha + beta)\r\n            print(fea1 + '_' + fea2 + ' : finish!!!')\r\n            valid_df = pd.merge(valid_df, temp_pivot_table, on=[fea1, fea2], how='left')\r\n    return valid_df\r\n\r\n\r\n\r\n\r\ndef getfeature(df):\r\n    df.drop_duplicates(inplace=True)\r\n    \r\n    ite = 1\r\n    since = time.time()\r\n    \r\n    #query 长度\r\n    df['query_len'] = df.apply(lambda x: len(x[1].split(' ')), axis=1)\r\n    df['query_len'] = df['query_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query_unique 长度\r\n    def get_query_unique(x):\r\n        a = len(set(x[1].split(' '))-set(x[3].split(' ')))\r\n        return a\r\n    \r\n    df['query_unique'] = df.apply(get_query_unique, axis=1)\r\n    df['query_unique'] = df['query_unique'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #title 长度\r\n    df['title_len'] = df.apply(lambda x: len(x[3].split(' ')), axis=1)\r\n    df['title_len'] = df['title_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #title_unique 长度\r\n    def get_query_unique(x):\r\n        a = len(set(x[3].split(' '))-set(x[1].split(' ')))\r\n        return a\r\n    df['title_unique'] = df.apply(get_query_unique, axis=1)\r\n    df['title_unique'] = df['title_unique'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #长度差、unique长度差\r\n    df['dif'] = (df['query_len'] - df['title_len']).abs()\r\n    df['dif'] = df['dif'].astype(np.int32)\r\n    df['dif_unique'] = (df['query_unique'] - df['title_unique']).abs()\r\n    df['dif_unique'] = df['dif_unique'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query_title_same_len query和title相同的长度\r\n    def query_title_same_len(x):\r\n        a = len(x[3].split(' ') and x[1].split(' '))\r\n        return a\r\n    df['query_title_same_len'] = df.apply(query_title_same_len, axis=1)\r\n    df['query_title_same_len'] = df['query_title_same_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n\r\n    #query和title相同的长度在query的比率\r\n    def samelen_query_rate(x):\r\n        a = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        return a/len(x[1].split(' '))\r\n    df['samelen_query_rate'] = df.apply(samelen_query_rate, axis=1)\r\n    df['samelen_query_rate'] = df['samelen_query_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query和title相同的长度在title的比率\r\n    def samelen_title_rate(x):\r\n        a = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        return a/len(x[3].split(' '))\r\n    df['samelen_title_rate'] = df.apply(samelen_title_rate, axis=1)\r\n    df['samelen_title_rate'] = df['samelen_title_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    \r\n    #query和title总长度\r\n    def q_t_all_len(x):\r\n        a = len(x[1].split(' ')) + len(x[3].split(' '))\r\n        return a\r\n    df['q_t_all_len'] = df.apply(q_t_all_len, axis=1)\r\n    df['q_t_all_len'] = df['q_t_all_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query和title的unique长度\r\n    def q_t_unique_len(x):\r\n        a = len(set(x[1].split(' ') + x[3].split(' ')))\r\n        return a\r\n    df['q_t_unique_len'] = df.apply(q_t_unique_len, axis=1)\r\n    df['q_t_unique_len'] = df['q_t_unique_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n\r\n    \r\n    #query和title总词数\r\n    def q_t_all_word_len(x):\r\n        a = len(set(x[1].split(' ')) | set(x[3].split(' ')))\r\n        return a\r\n    df['q_t_all_word_len'] = df.apply(q_t_all_word_len, axis=1)\r\n    df['q_t_all_word_len'] = df['q_t_all_word_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query和title不同词数\r\n    df['q_t_diff_len'] = df.apply(lambda x: len(set(x[1].split(' '))^set(x[3].split(' '))), axis=1)\r\n    df['q_t_diff_len'] = df['q_t_diff_len'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query和title不同词数在query的比率\r\n    def q_t_diff_q_rate(x):\r\n        a = len(set(x[1].split(' '))^set(x[3].split(' ')))\r\n        return np.float32(a/len(x[1].split(' ')))\r\n    df['q_t_diff_q_rate'] = df.apply(q_t_diff_q_rate, axis=1)\r\n    df['q_t_diff_q_rate'] = df['q_t_diff_q_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query和title不同词数在title的比率\r\n    def q_t_diff_t_rate(x):\r\n        a = len(set(x[1].split(' '))^set(x[3].split(' ')))\r\n        return np.float32(a/len(x[3].split(' ')))\r\n    df['q_t_diff_t_rate'] = df.apply(q_t_diff_t_rate, axis=1)\r\n    df['q_t_diff_t_rate'] = df['q_t_diff_t_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n\r\n    \r\n    #query在title中的不同词数\r\n    def query_diff_title(x):\r\n        a = len(set(x[1].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.int32(a)\r\n    df['query_diff_title'] = df.apply(query_diff_title, axis=1)\r\n    df['query_diff_title'] = df['query_diff_title'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query在title中的不同词数在query的比率\r\n    def query_diff_title_rate(x):\r\n        a = len(set(x[1].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.float32(a/len(x[1].split(' ')))\r\n    df['query_diff_title_rate'] = df.apply(query_diff_title_rate, axis=1)\r\n    df['query_diff_title_rate'] = df['query_diff_title_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #query在title中的不同词数在title的比率\r\n    def query_diff_title_rate(x):\r\n        a = len(set(x[1].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.float32(a/len(x[3].split(' ')))\r\n    df['query_diff_title_rate'] = df.apply(query_diff_title_rate, axis=1)\r\n    df['query_diff_title_rate'] = df['query_diff_title_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #title在qurey中的不同词数\r\n    def title_diff_query(x):\r\n        a = len(set(x[3].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.int32(a)\r\n    df['title_diff_query'] = df.apply(title_diff_query, axis=1)\r\n    df['title_diff_query'] = df['title_diff_query'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #title在qurey中的不同词数在query的比率\r\n    def title_diff_query_rate(x):\r\n        a = len(set(x[3].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.float32(a/len(x[1].split(' ')))\r\n    df['title_diff_query_rate'] = df.apply(title_diff_query_rate, axis=1)\r\n    df['title_diff_query_rate'] = df['title_diff_query_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #title在qurey中的不同词数在title的比率\r\n    def title_diff_query_rate(x):\r\n        a = len(set(x[3].split(' ')) - (set(x[1].split(' ')) & set(x[3].split(' '))))\r\n        return np.float32(a/len(x[3].split(' ')))\r\n    df['title_diff_query_rate'] = df.apply(title_diff_query_rate, axis=1)\r\n    df['title_diff_query_rate'] = df['title_diff_query_rate'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #Dice系数\r\n    def dice(x):\r\n        a1 = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        a2 = len(set(x[1].split(' '))) + len(set(x[3].split(' ')))\r\n        return np.float32(2*a1/a2)\r\n    df['dice'] = df.apply(dice, axis=1)\r\n    df['dice'] = df['dice'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #jaccord系数\r\n    def jaccord(x):\r\n        a1 = len(set(x[1].split(' ')) & set(x[3].split(' ')))\r\n        a2 = len(set(x[1].split(' ')) | set(x[3].split(' ')))\r\n        return np.float32(a1/a2)\r\n    df['jaccord'] = df.apply(jaccord, axis=1)\r\n    df['jaccord'] = df['jaccord'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #同一个query出现次数\r\n    df['query_count'] = df.groupby(1)[1].transform('count')\r\n    df['query_count'] = df['query_count'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #同一个title出现的次数\r\n    df['title_count'] = df.groupby(3)[3].transform('count')\r\n    df['title_count'] = df['title_count'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    \r\n#    #query和labe的交叉点击\r\n#    df['query_label_click'] = df.groupby([1, 4])[1].transform('count')\r\n#    df['query_label_click'] = df['query_label_click'].astype(np.int32)\r\n#    \r\n#    #title和labe的交叉点击\r\n#    df['title_label_click'] = df.groupby([3, 4])[3].transform('count')\r\n#    df['title_label_click'] = df['title_label_click'].astype(np.int32)\r\n    \r\n    \r\n    #lcseque_lens\r\n    df['lcseque_lens'] = df.apply(lambda x: lcseque_lens(x[1], x[3]), axis=1)\r\n    df['lcseque_lens'] = df['lcseque_lens'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #lcsubstr_lens\r\n    df['lcsubstr_lens'] = df.apply(lambda x: lcsubstr_lens(x[1], x[3]), axis=1)\r\n    df['lcsubstr_lens'] = df['lcsubstr_lens'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n\r\n    #ratio\r\n    df['ratio'] = df.apply(lambda x: Levenshtein.ratio(x[1], x[3]), axis=1)\r\n    df['ratio'] = df['ratio'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    \r\n\r\n    #Jaro距离\r\n    df['jaro'] = df.apply(lambda x: Levenshtein.jaro(x[1], x[3]), axis=1)\r\n    df['jaro'] = df['jaro'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    #jaro_winkler\r\n    df['jaro_winkler'] = df.apply(lambda x: Levenshtein.jaro_winkler(x[1], x[3]), axis=1)\r\n    df['jaro_winkler'] = df['jaro_winkler'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n\r\n\r\n    #余弦相似度\r\n    #使用tfidf向量求相似度\r\n    ct = TfidfVectorizer()\r\n    def cosine_similarity(x):\r\n        ct.fit([x[1] + ' ' + x[3]])\r\n        vect1 = ct.transform([x[1]]).toarray()[0]\r\n        vect2 = ct.transform([x[3]]).toarray()[0]\r\n        sum = 0\r\n        sq1 = 0\r\n        sq2 = 0\r\n        for i in range(len(vect1)):\r\n            sum += vect1[i] * vect2[i]\r\n            sq1 += pow(vect1[i], 2)\r\n            sq2 += pow(vect2[i], 2)\r\n        try:\r\n            result = round(float(sum) / (math.sqrt(sq1) * math.sqrt(sq2)), 2)\r\n        except ZeroDivisionError:\r\n            result = 0.0\r\n        return result \r\n    df['cosine_similarity_tfidf'] = df.apply(lambda x: compute_cosine(x[1], x[3]), axis=1)\r\n    df['cosine_similarity_tfidf'] = df['cosine_similarity_tfidf'].astype(np.float32)\r\n    #使用词频向量求相似度\r\n    df['cosine_similarity'] = df.apply(lambda x: compute_cosine(x[1], x[3]), axis=1)\r\n    df['cosine_similarity'] = df['cosine_similarity'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    \r\n    #编辑距离\r\n    df['levenshtein'] = df.apply(lambda x: Levenshtein.distance(x[1], x[3]), axis=1)\r\n    df['levenshtein'] = df['levenshtein'].astype(np.int32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    \r\n    #皮尔逊相关系数Pehrson\r\n    df['Pehrson'] = df.apply(lambda x:Pehrson(x[1], x[3]), axis=1)\r\n    df['Pehrson'] = df['Pehrson'].fillna(df['Pehrson'].mean())\r\n    df['Pehrson'] = df['Pehrson'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{}complete in {:.0f}m {:.0f}s'.format(ite, time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    \r\n    #计算两个字符串list的相似度\r\n    df['list_dis'] = df.apply(lambda x: Levenshtein.seqratio(x[1].split(' '), x[3].split(' ')), axis=1)\r\n    df['list_dis'] = df['list_dis'].astype(np.float32)\r\n    \r\n    time_elapsed = time.time() - since#######################################\r\n    print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n    ite += 1\r\n    \r\n    \r\n#     #Word2vec计算两个list的相似度\r\n#     model_word = Word2Vec.load('./word2vec_groupby.model')\r\n#     def word2vec_similar(x):\r\n#         a = x[1].split(' ')\r\n#         a = [i for i in a if i in model_word]\r\n#         b = x[3].split(' ')\r\n#         b = [i for i in b if i in model_word]  \r\n#         if len(a) == 0 or len(b) == 0:\r\n#               return 0\r\n#         return model_word.n_similarity(a, b)\r\n#     df['word_list_similarity'] = df.apply(lambda x: word2vec_similar(x), axis=1)\r\n    \r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n    \r\n    \r\n#     #使用word2vec求句向量的均值和最大值\r\n#     def getvec(x):\r\n#         a = 0\r\n#         n = 0\r\n#         for i in x:\r\n#             try:\r\n#                 a += model_word[i]\r\n#                 n += 1\r\n#             except:\r\n#                 pass\r\n#         a = np.array(a)/n\r\n#         a = a.astype(np.float32)\r\n#         return a\r\n#         #query\r\n#         df['word2vec_query_mean'] = df.apply(lambda x: model_word.getvec(x[1].split(' ')).mean(), axis=1)\r\n#         df['word2vec_query_max'] = df.apply(lambda x: model_word.getvec(x[1].split(' ')).max(), axis=1)\r\n#         #title\r\n#         df['word2vec_title_mean'] = df.apply(lambda x: model_word.getvec(x[3].split(' ')).mean(), axis=1)\r\n#         df['word2vec_title_max'] = df.apply(lambda x: model_word.getvec(x[3].split(' ')).max(), axis=1)\r\n#         #query+title\r\n#         df['word2vec_query_title_mean'] = df.apply(lambda x: model_word.getvec(x[1].split(' ') + x[3].split(' ')).mean(), axis=1)\r\n#         df['word2vec_query_title_max'] = df.apply(lambda x: model_word.getvec(x[1].split(' ') + x[3].split(' ')).max(), axis=1)\r\n\r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n    \r\n    \r\n# #    #使用doc2vec求句向量的均值和最大值\r\n# #    model_doc = Doc2Vec.load('../1.model')\r\n# #    #query\r\n# #    df['docvec_query_mean'] = df.apply(lambda x: model_doc.infer_vector(x[1].split(' ')).mean(), axis=1)\r\n# #    df['docvec_query_max'] = df.apply(lambda x: model_doc.infer_vector(x[1].split(' ')).max(), axis=1)\r\n# #    #title\r\n# #    df['docvec_title_mean'] = df.apply(lambda x: model_doc.infer_vector(x[3].split(' ')).mean(), axis=1)\r\n# #    df['docvec_title_max'] = df.apply(lambda x: model_doc.infer_vector(x[3].split(' ')).max(), axis=1)\r\n# #    #query+title\r\n# #    df['docvec_query_title_mean'] = df.apply(lambda x: model_doc.infer_vector(x[1].split(' ') + x[3].split(' ')).mean(), axis=1)\r\n# #    df['docvec_query_title_max'] = df.apply(lambda x: model_doc.infer_vector(x[1].split(' ') + x[3].split(' ')).max(), axis=1)\r\n\r\n\r\n# #    #Doc2vec计算两个list的相似度\r\n# #    def doc2vec_similar(x):\r\n# #        a = x[1].split(' ')\r\n# #        a = [i for i in a if i in model_doc]\r\n# #        b = x[3].split(' ')\r\n# #        b = [i for i in b if i in model_doc]  \r\n# #        return model_doc.n_similarity(a, b)\r\n# #    df['doc_list_similarity'] = df.apply(lambda x: doc2vec_similar(x), axis=1)\r\n    \r\n\r\n# #    #doc2vec和word2vec分别得到相似度乘积\r\n# #    df['doc_word_list_similarity_product'] = df['word_list_similarity'] * df['doc_list_similarity']\r\n    \r\n\r\n#     #jaro_winkler和jaro的均值、乘积、差值\r\n#     df['jaroWinkler_jaro_mean'] = (df['jaro'] + df['jaro_winkler'])/2\r\n#     df['jaroWinkler_jaro_product'] = df['jaro'] * df['jaro_winkler']\r\n#     df['jaroWinkler_jaro_subtraction'] = (df['jaro'] - df['jaro_winkler']).abs()\r\n    \r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n      \r\n    \r\n#     #Pehrson和cosine_similarity的均值、乘积\r\n#     df['cosine_similarity_Pehrson_mean'] = (df['Pehrson'] + df['cosine_similarity'])/2\r\n#     df['cosine_similarity_Pehrson_product'] = df['Pehrson'] * df['cosine_similarity']\r\n#     df['cosine_similarity_Pehrson_subtraction'] = (df['Pehrson'] * df['cosine_similarity']).abs()\r\n    \r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n    \r\n    \r\n# #    #TF系数\r\n# #    #max\r\n#     def tf(x):\r\n#         x = x.split(' ')\r\n#         x = pd.Series(x).value_counts().values\r\n#         x = x/x.sum()\r\n#         return x\r\n# #    df['query_tf_max'] = df.apply(lambda x: tf(x[1]).max(), axis=1)\r\n#     df['title_tf_max'] = df.apply(lambda x: tf(x[3]).max(), axis=1)\r\n#     df['query_title_tf_max'] = df.apply(lambda x: tf(x[1] + ' ' + x[3]).max(), axis=1)\r\n#     #mean\r\n# #    df['query_tf_mean'] = df.apply(lambda x: tf(x[1]).mean(), axis=1)\r\n#     df['title_tf_mean'] = df.apply(lambda x: tf(x[3]).mean(), axis=1)\r\n#     df['query_title_tf_mean'] = df.apply(lambda x: tf(x[1] + ' ' + x[3]).mean(), axis=1)\r\n    \r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n    \r\n#     #idf系数\r\n#     #max\r\n#     def idf(x):\r\n#         x = x.split(' ')\r\n#         l = len(x)\r\n#         x = pd.Series(x).value_counts().values\r\n#         x = np.log10(l/x)\r\n#         return x\r\n# #    df['query_idf_max'] = df.apply(lambda x: idf(x[1]).max(), axis=1)\r\n#     df['title_idf_max'] = df.apply(lambda x: idf(x[3]).max(), axis=1)\r\n#     df['query_title_idf_max'] = df.apply(lambda x: idf(x[1] + ' ' + x[3]).max(), axis=1)\r\n#     #mean\r\n# #    df['query_idf_mean'] = df.apply(lambda x: idf(x[1]).mean(), axis=1)\r\n#     df['title_idf_mean'] = df.apply(lambda x: idf(x[3]).mean(), axis=1)\r\n#     df['query_title_idf_mean'] = df.apply(lambda x: idf(x[1] + ' ' + x[3]).mean(), axis=1)\r\n         \r\n    \r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n      \r\n#     #TF*IDF#####################################(40)\r\n#     def tfidf(x):\r\n#         t = tf(x)\r\n#         i = idf(x)\r\n#         ti = t*i\r\n#         return ti\r\n#     #max\r\n# #    df['query_tfidf_max'] = df.apply(lambda x: tfidf(x[1]).max(), axis=1)\r\n#     df['title_tfidf_max'] = df.apply(lambda x: tfidf(x[3]).max(), axis=1)\r\n#     df['query_title_tfidf_max'] = df.apply(lambda x: tfidf(x[1] + ' ' + x[3]).max(), axis=1)\r\n#     #mean\r\n# #    df['query_tfidf_mean'] = df.apply(lambda x: tfidf(x[1]).mean(), axis=1)\r\n#     df['title_tfidf_mean'] = df.apply(lambda x: tfidf(x[3]).mean(), axis=1)\r\n#     df['query_title_tfidf_mean'] = df.apply(lambda x: tfidf(x[1] + ' ' + x[3]).mean(), axis=1)\r\n    \r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n           \r\n\r\n    \r\n#     #每句的tfidf最大值，平均值\r\n#     t = TfidfVectorizer(ngram_range=(1, 2), analyzer='char',binary=True)\r\n#     title = t.fit_transform(df[3]).toarray()\r\n#     df['title_tfidfvec_mean'] = title.mean(axis=1)\r\n#     df['title_tfidfvec_max'] = title.max(axis=1)\r\n#     del title\r\n#     q_t = t.fit_transform(df[1]+' '+df[3]).toarray()\r\n#     df['query_title_tfidfvec_mean'] = q_t.mean(axis=1)\r\n#     df['query_title_tfidfvec_max'] = q_t.max(axis=1)\r\n#     del q_t\r\n    \r\n#     time_elapsed = time.time() - since#######################################\r\n#     print('{} {} complete in {:.0f}m {:.0f}s'.format(ite, df.columns[-1], time_elapsed // 60, time_elapsed % 60))\r\n#     ite += 1\r\n    \r\n    \r\n###############################################################################    \r\n#    skf = StratifiedKFold(n_splits=5, random_state=2019, shuffle=True)\r\n#    #单维度转化率特征\r\n#    single_fea_set = ['query', 'title']\r\n#    df = get_single_dimension_rate_train_feature(df, single_fea_set)\r\n#    \r\n#    #交叉转化率特征\r\n#    jiaoch_fea_set = ['query', 'title']\r\n#    df = get_jiaocha_dimension_rate_train_feature(df, jiaoch_fea_set)\r\n###############################################################################\r\n    \r\n    \r\n\r\n    # try:\r\n    #     df.drop([0, 1, 2, 3, 4], axis=1, inplace=True)\r\n    # except:\r\n    #     df.drop([0, 1, 2, 3], axis=1, inplace=True)\r\n    \r\n\r\n    return df","execution_count":9},{"metadata":{"id":"973D9994DFEE4AFE866B9183DEF99871","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"x = 1\nfor df in train:\n    if x == 1:\n        df_train = df.copy()\n    if x == 10:\n        df = pd.concat((df, df_train))\n        break\n    print(x)\n    x += 1\ndel df_train\ngc.collect()","execution_count":9},{"metadata":{"id":"71CCEEE87C8B481888C41A131FD132B0","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"df['index'] = df.index\n\ny_train = df['label']\ny_train.to_csv('feature_label_First1ELast1E_test.txt', index=False)\n\n# #单维度转化率\n# single_fea_set = ['query', 'title']\n# #训练集\n# df = feature.get_single_dimension_rate_train_feature(df, single_fea_set)\n# #测试集\n# test = feature.get_single_dimension_rate_test_feature(df, test, single_fea_set)\n\n# #交叉维度转化率\n# jiaoch_fea_set = ['query', 'title']\n# #训练集\n# df = feature.get_jiaocha_dimension_rate_train_feature(df, jiaoch_fea_set)\n# #测试集\n# test = feature.get_jiaocha_dimension_rate_test_feature(df, test, jiaoch_fea_set)\n\ndel y_train\ngc.collect()","execution_count":10},{"metadata":{"id":"E0A2925D03784B0981AB65AFE8F8DB74","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"#特征工程（统计特征，距离特征，相似度特征）\ndf.index = df['index']\ndf.drop(['index', 'label'], axis=1, inplace=True)\n\ndf = pd.concat((df, test))\ndf.fillna(-1, inplace=True)\n\ndf = df.rename(columns = {'query_id':0, 'query':1, 'title_id':2, 'title':3})","execution_count":11},{"metadata":{"id":"7D1BD7F33AFF473CADA7701187E5CE84","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":12},{"metadata":{"id":"B1F74662ADED4E8E82DF99E80D06A6E7","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"if __name__ == '__main__':\n    p = multiprocessing.Pool(10)\n    process_list = []\n    for i in range(10):  \n        # p = multiprocessing.Process(target=getfeature,args=(df.iloc[i*22000000:(i+1)*22000000],))\n        result = p.apply_async(getfeature,args=(df.iloc[i*2200:(i+1)*2200],))#实例化进程对象\n        process_list.append(result)\n\n    p.close()\n    p.join()\n        \n\n","execution_count":29},{"metadata":{"id":"61FFF8FF71954F528972FC6EA3A0C661","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"process_list[1].get()","execution_count":32},{"metadata":{"id":"23904909BEC9418FA420CEFA1E7F82DC","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"df = pd.concat([i.get() for i in process_list])","execution_count":34},{"metadata":{"id":"7B2010BE8B2A42788E03968BAF431C1F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"df","execution_count":36},{"metadata":{"id":"D3DB3989E63343E8855C2C14A9C307A1","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"df.to_csv('feature_First1ELast1E_test.txt', index=False)","execution_count":null},{"metadata":{"id":"EFF53F3F70474E87A180BF939611BF6C"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}